{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSNK7duj5SeU"
   },
   "source": [
    "# 00. PyTorch 基础\n",
    "\n",
    "## 什么是 PyTorch？\n",
    "\n",
    "[PyTorch](https://pytorch.org/) 是一个开源的机器学习和深度学习框架。\n",
    "\n",
    "## PyTorch 能做什么？\n",
    "\n",
    "PyTorch 允许您使用 Python 代码来操作和处理数据以及编写机器学习算法。\n",
    "\n",
    "## 谁在使用 PyTorch？\n",
    "\n",
    "许多全球顶尖的科技公司，如 [Meta（Facebook）](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/)、Tesla 和 Microsoft 以及人工智能研究公司如 [OpenAI](https://openai.com/blog/openai-pytorch/) 都使用 PyTorch 来支持研究并将机器学习应用于产品中。\n",
    "\n",
    "例如，特斯拉的 AI 负责人 Andrej Karpathy 曾在多个场合（[PyTorch DevCon 2019](https://youtu.be/oBklltKXtDE)，[Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=2904)）讲解特斯拉如何利用 PyTorch 来支持自动驾驶的计算机视觉模型。\n",
    "\n",
    "PyTorch 也被应用在其他行业中，例如农业，[用于驱动拖拉机的计算机视觉系统](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)。\n",
    "\n",
    "## 为什么要使用 PyTorch？\n",
    "\n",
    "机器学习研究人员非常喜欢使用 PyTorch。截至 2022 年 2 月，PyTorch 是 [Papers With Code](https://paperswithcode.com/trends) 上使用最广泛的深度学习框架（该网站追踪机器学习研究论文及其代码仓库）。\n",
    "\n",
    "PyTorch 还提供了许多便利功能，如 GPU 加速（让代码运行更快），以便您可以专注于数据操作和算法编写，而 PyTorch 会确保其运行速度足够快。\n",
    "\n",
    "如果像 Tesla 和 Meta（Facebook）这样的公司使用 PyTorch 来构建用于支持数百个应用程序、驱动数千辆汽车并向数十亿人提供内容的模型，那么在开发方面 PyTorch 也是非常可靠的。\n",
    "\n",
    "## 本模块将介绍的内容\n",
    "\n",
    "本课程分为多个不同的部分（笔记本）。每个笔记本涵盖了 PyTorch 中的重要概念。\n",
    "\n",
    "后续的笔记本将建立在之前内容的基础上（编号从 00、01、02 开始，直到课程结束）。\n",
    "\n",
    "此笔记本介绍机器学习和深度学习的基本构建块，即张量（tensor）。\n",
    "\n",
    "具体内容包括：\n",
    "\n",
    "| **主题** | **内容** |\n",
    "| ----- | ----- |\n",
    "| **张量简介** | 张量是所有机器学习和深度学习的基本构建块。 |\n",
    "| **创建张量** | 张量可以表示几乎任何类型的数据（图像、文字、数字表格）。 |\n",
    "| **从张量中获取信息** | 如果可以将信息放入张量中，那么您也会希望将其取出。 |\n",
    "| **张量操作** | 机器学习算法（如神经网络）涉及以多种方式操作张量，例如加法、乘法、组合等。 | \n",
    "| **处理张量形状** | 机器学习中最常见的问题之一是处理形状不匹配（试图将不匹配形状的张量组合）。 |\n",
    "| **张量索引** | 如果您使用过 Python 列表或 NumPy 数组的索引，张量的索引非常类似，只是维度可能更多。 |\n",
    "| **混合使用 PyTorch 张量和 NumPy** | PyTorch 使用张量（[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)），NumPy 喜欢数组（[`np.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)），有时您可能需要两者混合使用。 | \n",
    "| **可重复性** | 机器学习是非常实验性的，并且通常涉及大量*随机性*，有时您会希望这种*随机性*可以不那么随机。 |\n",
    "| **在 GPU 上运行张量** | GPU（图形处理单元）可以加速代码运行，PyTorch 使得在 GPU 上运行代码变得简单。\n",
    "\n",
    "## 哪里可以获得帮助？\n",
    "\n",
    "本课程的所有资料都在 [GitHub 上](https://github.com/mrdbourke/pytorch-deep-learning)。\n",
    "\n",
    "如果遇到问题，您也可以在[讨论页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions)提问。\n",
    "\n",
    "此外，还有 [PyTorch 开发者论坛](https://discuss.pytorch.org/)，这是一个非常有帮助的 PyTorch 讨论平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v3iRCRUTGeu"
   },
   "source": [
    "## 导入 PyTorch\n",
    "\n",
    "> **注意:** 在运行此笔记本中的任何代码之前，您应该先完成 [PyTorch 的安装步骤](https://pytorch.org/get-started/locally/)。\n",
    ">\n",
    "> 但是，**如果您在 Google Colab 上运行**，一切应该都能正常工作（Google Colab 已预装 PyTorch 和其他库）。\n",
    "\n",
    "让我们从导入 PyTorch 并检查我们使用的版本开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1VxEOik46Y4i",
    "outputId": "f3141076-29bc-4600-c1c3-1586b1fe2292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SqvI4S9TGew"
   },
   "source": [
    "很好，看起来我们已经安装了 PyTorch 1.10.0+。\n",
    "\n",
    "这意味着如果您正在学习这些资料，大多数内容将与 PyTorch 1.10.0+ 兼容，但如果您的版本号比这高很多，您可能会注意到一些不一致之处。\n",
    "\n",
    "如果遇到任何问题，请在课程的 [GitHub 讨论页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions)上发布您的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-33BKR16iWc"
   },
   "source": [
    "## 张量简介\n",
    "\n",
    "现在我们已经导入了 PyTorch，是时候了解张量了。\n",
    "\n",
    "张量是机器学习的基本构建块。\n",
    "\n",
    "它们的作用是以数值方式表示数据。\n",
    "\n",
    "例如，您可以将图像表示为形状为 `[3, 224, 224]` 的张量，这表示 `[颜色通道, 高度, 宽度]`，也就是说该图像有 `3` 个颜色通道（红色、绿色、蓝色），高度为 `224` 像素，宽度为 `224` 像素。\n",
    "\n",
    "![从输入图像到张量表示的图像示例，图像被分解为3个颜色通道，以及表示高度和宽度的数值](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-tensor-shape-example-of-image.png)\n",
    "\n",
    "在张量语言中（描述张量的术语），该张量有三个维度，分别对应 `颜色通道`、`高度` 和 `宽度`。\n",
    "\n",
    "但这有点超前了。\n",
    "\n",
    "让我们通过编码进一步学习张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFF0N2TU7S7Q"
   },
   "source": [
    "### 创建张量\n",
    "\n",
    "PyTorch 对张量情有独钟，以至于专门有一整页文档来介绍 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) 类。\n",
    "\n",
    "你的第一项作业是花 10 分钟[阅读 `torch.Tensor` 的文档](https://pytorch.org/docs/stable/tensors.html)。不过这个可以稍后再做。\n",
    "\n",
    "现在，让我们开始编码。\n",
    "\n",
    "我们将首先创建一个 **标量**。\n",
    "\n",
    "标量是一个单一的数字，用张量语言来说，它是一个零维张量。\n",
    "\n",
    "> **注意:** 这是本课程的一个趋势。我们将专注于编写特定代码。但我会经常安排练习，涉及阅读和熟悉 PyTorch 文档。毕竟，完成本课程后，您很可能还会想了解更多内容，而文档是您经常会查找的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUDgG2zk7Us5",
    "outputId": "0ac22bd2-16bc-4307-f312-31ae89d6c375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqSuhW7rTGey"
   },
   "source": [
    "看到上面输出的 `tensor(7)` 了吗？\n",
    "\n",
    "这意味着虽然 `scalar` 是一个单一的数字，但它的类型是 `torch.Tensor`。\n",
    "\n",
    "我们可以使用 `ndim` 属性来检查张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV98Yz868bav",
    "outputId": "502a625e-ff3c-4fc4-b523-f7634ea82128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO2YW_QGTGez"
   },
   "source": [
    "如果我们想从张量中取出这个数字怎么办？\n",
    "\n",
    "也就是说，将它从 `torch.Tensor` 转换为一个 Python 整数？\n",
    "\n",
    "我们可以使用 `item()` 方法来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k4cyKumPfbE",
    "outputId": "1f6a7916-0c7c-403f-8ebd-875454a94470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取张量中的 Python 数字（仅适用于单元素张量）\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs7ulrATGe0"
   },
   "source": [
    "好的，现在让我们看看 **向量**。\n",
    "\n",
    "向量是一个一维张量，但可以包含多个数字。\n",
    "\n",
    "例如，您可以用一个向量 `[3, 2]` 来描述您家的 `[卧室数, 浴室数]`，或者用 `[3, 2, 2]` 来描述 `[卧室数, 浴室数, 车位数]`。\n",
    "\n",
    "这里的重要趋势是，向量具有灵活的表示能力（张量也是如此）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IZF6ASs8QH9",
    "outputId": "e556ed2a-e58a-440f-b103-0f06c91bc75c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXxRUUW2TGe1"
   },
   "source": [
    "很好，`vector` 现在包含了两个 7，这是我最喜欢的数字。\n",
    "\n",
    "你认为它会有多少个维度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03hm3VVv8kr4",
    "outputId": "2035bb26-0189-4b28-fa02-34220d44677f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查 vector 的维数\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0VYvSGbTGe1"
   },
   "source": [
    "嗯，这很奇怪，`vector` 包含两个数字，但只有一个维度。\n",
    "\n",
    "我来告诉你一个小技巧。\n",
    "\n",
    "在 PyTorch 中，你可以通过张量外侧方括号的数量 (`[`) 来判断它的维数，只需要数一边即可。\n",
    "\n",
    "`vector` 有多少个方括号？\n",
    "\n",
    "张量的另一个重要概念是其 `shape` 属性。`shape` 告诉你其中元素的排列方式。\n",
    "\n",
    "让我们来看看 `vector` 的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zREV1bDTGe2",
    "outputId": "2a6e7ceb-7eb2-422b-b006-2c6e4825272f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查 vector 的形状\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aWKppNyTGe2"
   },
   "source": [
    "上面的代码返回了 `torch.Size([2])`，这意味着我们的向量的形状是 `[2]`。这是因为我们在方括号中放置了两个元素 (`[7, 7]`)。\n",
    "\n",
    "现在让我们来看一个 **矩阵**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5iNwCYL8QO9",
    "outputId": "88fc63a7-4130-4c7a-a574-c61e85d2e99e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵\n",
    "MATRIX = torch.tensor([[7, 8], \n",
    "                       [9, 10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3U1bCdjTGe3"
   },
   "source": [
    "哇！更多的数字！矩阵和向量一样灵活，只不过它们多了一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LREUbeb8r8j",
    "outputId": "636246b0-b109-472a-c6d5-8601a9e08654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查维数\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhXXgq-dTGe3"
   },
   "source": [
    "`MATRIX` 有两个维度（你数过一边外侧的方括号数量了吗？）。\n",
    "\n",
    "你认为它的 `shape` 会是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TL26I31TGe3",
    "outputId": "f05ec0b6-0bc1-4381-9474-56cbe6c67139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvLpUvrKTGe4"
   },
   "source": [
    "我们得到输出 `torch.Size([2, 2])`，因为 `MATRIX` 的深度为 2，宽度为 2。\n",
    "\n",
    "现在我们来创建一个 **张量**，怎么样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEMDQr188QWW",
    "outputId": "4230e6bd-1844-4210-eea8-245bb8b8b265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmJKkXD7TGe4"
   },
   "source": [
    "这是一个漂亮的张量。\n",
    "\n",
    "我想强调一下，张量几乎可以表示任何东西。\n",
    "\n",
    "我们刚刚创建的这个张量可以代表牛排和杏仁黄油店的销售数据（我最喜欢的两种食物）。\n",
    "\n",
    "![在 Google Sheets 中显示的简单张量，包含星期几、牛排销售量和杏仁黄油销售量](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_simple_tensor.png)\n",
    "\n",
    "你认为它有多少维？（提示：使用数方括号的小技巧）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dhuEsjS8QcT",
    "outputId": "7a45df1b-fc32-4cc5-e330-527c6ef7ba5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查 TENSOR 的维数\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln9dys5VTGe4"
   },
   "source": [
    "它的形状是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdVv4iNRTGe5",
    "outputId": "d8ac706c-020b-4926-b145-d44e41f35e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查 TENSOR 的形状\n",
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxk8GU7oTGe5"
   },
   "source": [
    "好的，它输出 `torch.Size([1, 3, 3])`。\n",
    "\n",
    "维度是从外到内排列的。\n",
    "\n",
    "这意味着有一个 3x3 的维度。\n",
    "\n",
    "![不同张量维度的示例](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n",
    "\n",
    "> **注意:** 你可能注意到我用小写字母表示 `scalar` 和 `vector`，而用大写字母表示 `MATRIX` 和 `TENSOR`。这是故意的。实际上，标量和向量通常用小写字母表示，比如 `y` 或 `a`；矩阵和张量通常用大写字母表示，比如 `X` 或 `W`。\n",
    ">\n",
    "> 你还可能注意到矩阵和张量的名称有时可以互换使用。这是常见的情况。在 PyTorch 中，你通常处理的是 `torch.Tensor`（因此称为张量），但其中的形状和维度决定了它实际上是什么。\n",
    "\n",
    "让我们总结一下。\n",
    "\n",
    "| 名称 | 它是什么？ | 维数 | 通常使用的大小写（示例） |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **scalar** | 一个单独的数字 | 0 | 小写 (`a`) | \n",
    "| **vector** | 有方向的数字（例如带有方向的风速），也可以包含多个数字 | 1 | 小写 (`y`) |\n",
    "| **matrix** | 一个二维数字数组 | 2 | 大写 (`Q`) |\n",
    "| **tensor** | 一个 n 维的数字数组 | 可以是任意维数，0 维张量是标量，1 维张量是向量 | 大写 (`X`) | \n",
    "\n",
    "![标量、向量、矩阵、张量及其示例](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dms7G4nkTGe5"
   },
   "source": [
    "### 随机张量\n",
    "\n",
    "我们已经了解了张量表示某种形式的数据。\n",
    "\n",
    "而机器学习模型（如神经网络）会操作张量并在其中寻找模式。\n",
    "\n",
    "但是在用 PyTorch 构建机器学习模型时，很少会手动创建张量（就像我们刚刚做的那样）。\n",
    "\n",
    "相反，机器学习模型通常从大量随机数构成的张量开始，并在处理数据的过程中调整这些随机数，以便更好地表示数据。\n",
    "\n",
    "本质上就是：\n",
    "\n",
    "`从随机数开始 -> 查看数据 -> 更新随机数 -> 查看数据 -> 更新随机数...`\n",
    "\n",
    "作为数据科学家，您可以定义机器学习模型的初始状态（初始化）、查看数据的方式（表示）以及如何更新（优化）其随机数。\n",
    "\n",
    "稍后我们将亲自操作这些步骤。\n",
    "\n",
    "现在，让我们看看如何创建一个包含随机数的张量。\n",
    "\n",
    "我们可以使用 [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html) 并传入 `size` 参数来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOJEtDx--GnK",
    "outputId": "2680d44b-e31c-4ab1-d5b1-c0cd76706a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2836, 0.6571, 0.5542, 0.3539],\n",
       "         [0.0596, 0.2641, 0.4576, 0.3692],\n",
       "         [0.5950, 0.6848, 0.5767, 0.6493]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个大小为 (3, 4) 的随机张量\n",
    "random_tensor = torch.rand(size=(3, 4))\n",
    "random_tensor, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wB1c_cXTGe5"
   },
   "source": [
    "`torch.rand()` 的灵活性在于我们可以将 `size` 调整为我们想要的任何大小。\n",
    "\n",
    "例如，假设您想要一个常见的图像形状的随机张量 `[224, 224, 3]`（`[高度, 宽度, 颜色通道]`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMF_NUp3Ym__",
    "outputId": "8346b853-0b1e-481a-d9ee-a410ee21bab0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个大小为 (224, 224, 3) 的随机张量\n",
    "random_image_size_tensor = torch.rand(size=(224, 224, 3))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MQNTY0eTGe6"
   },
   "source": [
    "### 零和一\n",
    "\n",
    "有时您可能只想用零或一来填充张量。\n",
    "\n",
    "这种情况在掩码处理中经常出现（比如用零来掩盖张量中的某些值，以便模型知道不学习这些值）。\n",
    "\n",
    "让我们使用 [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html) 创建一个全为零的张量。\n",
    "\n",
    "同样地，`size` 参数在这里也很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCzhd0hl9Vp6",
    "outputId": "9c8ec87f-d8c9-4751-a13e-6a5e986daaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个全为0的张量\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "zeros, zeros.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDQBZJRUZWTN"
   },
   "source": [
    "我们可以使用 [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html) 来创建一个全为一的张量，方法与创建全为零的张量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRe6sSXiTGe6",
    "outputId": "3f45b0b8-7f65-423d-c664-f5b5f7866fd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个全为1的张量\n",
    "ones = torch.ones(size=(3, 4))\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hib1NYrSarL2"
   },
   "source": [
    "### 创建一个范围和类似张量\n",
    "\n",
    "有时你可能想要一个数字范围，比如从 1 到 10 或从 0 到 100。\n",
    "\n",
    "你可以使用 `torch.arange(start, end, step)` 来实现。\n",
    "\n",
    "其中：\n",
    "* `start` = 范围的起始值（例如 0）\n",
    "* `end` = 范围的结束值（例如 10）\n",
    "* `step` = 每个值之间的步长（例如 1）\n",
    "\n",
    "> **注意:** 在 Python 中，你可以使用 `range()` 来创建一个范围。然而在 PyTorch 中，`torch.range()` 已被弃用，未来可能会出现错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IqUs81d9W4W",
    "outputId": "2a6f0c08-052e-4b36-b4eb-6a537239026f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/rnny0md942g8pkfqbsh4qgrm0000gn/T/ipykernel_14700/3973444322.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  zero_to_ten_deprecated = torch.range(0, 10)  # 注意：未来可能会返回错误\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 torch.arange()，torch.range() 已被弃用\n",
    "zero_to_ten_deprecated = torch.range(0, 10)  # 注意：未来可能会返回错误\n",
    "\n",
    "# 创建一个从 0 到 10 的值范围\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-bXf0Ugbh-D"
   },
   "source": [
    "有时你可能希望创建一个与另一个张量形状相同的特定类型的张量。\n",
    "\n",
    "例如，一个与先前张量形状相同的全零张量。\n",
    "\n",
    "要实现这一点，你可以使用 [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) 或 [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html)，它们分别返回一个与 `input` 形状相同的全零或全一张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvXwUut5BhHq",
    "outputId": "096b2f8e-8c21-4ace-97b9-c36b92b2fe77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以创建一个与另一个张量形状相同的全零张量\n",
    "ten_zeros = torch.zeros_like(input=zero_to_ten)  # 将具有相同的形状\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKZ6QlYTGe7"
   },
   "source": [
    "### 张量数据类型\n",
    "\n",
    "在 PyTorch 中有许多不同的[张量数据类型](https://pytorch.org/docs/stable/tensors.html#data-types)。\n",
    "\n",
    "有些专门用于 CPU，有些则更适合 GPU。\n",
    "\n",
    "了解每种类型需要一些时间。\n",
    "\n",
    "通常，如果你在任何地方看到 `torch.cuda`，说明该张量正在用于 GPU（因为 Nvidia GPU 使用名为 CUDA 的计算工具包）。\n",
    "\n",
    "最常见的类型（通常也是默认类型）是 `torch.float32` 或 `torch.float`。\n",
    "\n",
    "这被称为“32 位浮点”。\n",
    "\n",
    "此外，还有 16 位浮点（`torch.float16` 或 `torch.half`）和 64 位浮点（`torch.float64` 或 `torch.double`）。\n",
    "\n",
    "更复杂的是，还有 8 位、16 位、32 位和 64 位整数类型。\n",
    "\n",
    "甚至还有更多！\n",
    "\n",
    "> **注意:** 整数是像 `7` 这样的整数，而浮点数则带有小数，如 `7.0`。\n",
    "\n",
    "存在这么多不同数据类型的原因是为了**计算精度**。\n",
    "\n",
    "精度是描述一个数字的细节量。\n",
    "\n",
    "精度值越高（8、16、32），表达一个数字的细节就越多，使用的数据量也越大。\n",
    "\n",
    "在深度学习和数值计算中，这一点很重要，因为进行的计算操作非常多，精度越高，计算量也越大。\n",
    "\n",
    "因此，较低精度的数据类型计算速度通常更快，但在评估指标（如准确性）方面可能会牺牲一些性能（计算更快，但精度较低）。\n",
    "\n",
    "> **资源:** \n",
    "  * 请参阅 [PyTorch 文档中的所有张量数据类型列表](https://pytorch.org/docs/stable/tensors.html#data-types)。\n",
    "  * 阅读 [维基百科页面，概览计算中的精度](https://en.wikipedia.org/wiki/Precision_(computer_science))。\n",
    "\n",
    "让我们看看如何使用特定的数据类型创建张量。可以使用 `dtype` 参数来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3MoGnpw9XaF",
    "outputId": "61070939-8c52-4ac6-bed7-e64b3ce24615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量的默认数据类型是 float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None,  # 默认为 None，即 torch.float32 或传入的数据类型\n",
    "                               device=None,  # 默认为 None，即使用默认张量类型\n",
    "                               requires_grad=False)  # 如果为 True，则记录对张量执行的操作\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhP8kzDfe_ty"
   },
   "source": [
    "除了形状问题（张量形状不匹配）外，在 PyTorch 中你最常遇到的两个问题是数据类型和设备问题。\n",
    "\n",
    "例如，一个张量是 `torch.float32`，另一个是 `torch.float16`（PyTorch 通常要求张量具有相同的格式）。\n",
    "\n",
    "或者你的一个张量在 CPU 上，而另一个在 GPU 上（PyTorch 希望张量之间的计算在同一个设备上进行）。\n",
    "\n",
    "稍后我们会进一步了解设备相关的问题。\n",
    "\n",
    "现在让我们创建一个 `dtype=torch.float16` 的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKSuajld_09s",
    "outputId": "cbac29d9-3371-4fe1-b47c-3af4623b5fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16)  # torch.half 也可以使用\n",
    "\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUjkB2AX7Upz"
   },
   "source": [
    "## 从张量中获取信息\n",
    "\n",
    "一旦你创建了张量（或其他人或 PyTorch 模块为你创建了张量），你可能想从中获取一些信息。\n",
    "\n",
    "我们之前已经见过，但最常用的三个张量属性是：\n",
    "* `shape` - 张量的形状是什么？（某些操作需要特定的形状规则）\n",
    "* `dtype` - 张量中的元素是以什么数据类型存储的？\n",
    "* `device` - 张量存储在哪个设备上？（通常是 GPU 或 CPU）\n",
    "\n",
    "让我们创建一个随机张量并查看它的详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd_X4D0j7Umq",
    "outputId": "86045713-ab36-4c8e-840c-e788f80c5266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1577, 0.6866, 0.3661, 0.0622],\n",
      "        [0.1848, 0.9241, 0.0201, 0.7888],\n",
      "        [0.8203, 0.3314, 0.2637, 0.9357]])\n",
      "张量的形状: torch.Size([3, 4])\n",
      "张量的数据类型: torch.float32\n",
      "张量存储的设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量\n",
    "some_tensor = torch.rand(3, 4)\n",
    "\n",
    "# 查看张量的详细信息\n",
    "print(some_tensor)\n",
    "print(f\"张量的形状: {some_tensor.shape}\")\n",
    "print(f\"张量的数据类型: {some_tensor.dtype}\")\n",
    "print(f\"张量存储的设备: {some_tensor.device}\")  # 默认情况下是 CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45K-E5uPg6cj"
   },
   "source": [
    "> **注意:** 当你在 PyTorch 中遇到问题时，通常是与上述三个属性之一有关。因此，当错误消息出现时，可以唱一首小歌：“是什么、是什么、在哪里”：\n",
    "  * “*我的张量是什么形状？它们是什么数据类型，存储在哪里？是什么形状，是什么数据类型，在哪里，在哪里，在哪里*”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdiWvoAi7UjL"
   },
   "source": [
    "## 操作张量（张量运算）\n",
    "\n",
    "在深度学习中，数据（图像、文本、视频、音频、蛋白质结构等）被表示为张量。\n",
    "\n",
    "模型通过分析这些张量并在其上执行一系列操作（可能多达数百万次）来学习输入数据中的模式。\n",
    "\n",
    "这些操作通常是一种优雅的组合，包括：\n",
    "* 加法\n",
    "* 减法\n",
    "* 乘法（逐元素）\n",
    "* 除法\n",
    "* 矩阵乘法\n",
    "\n",
    "就是这些。当然，还有一些其他操作，但这些是神经网络的基本构建块。\n",
    "\n",
    "通过以正确的方式组合这些构建块，你可以创建出最复杂的神经网络（就像搭建乐高一样！）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk_6Dd7L7Uce"
   },
   "source": [
    "### 基本操作\n",
    "\n",
    "让我们从几个基本操作开始，加法（`+`）、减法（`-`）、乘法（`*`）。\n",
    "\n",
    "它们的工作方式正如你所想的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X71WpQoPD7a4",
    "outputId": "ab30f13e-fc67-4ae4-c5ce-1006410dba07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量并加上一个数值\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp4TlTWWEFeO",
    "outputId": "ce7d2296-881f-4eb3-802e-fd12bc25d6ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乘以 10\n",
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1VEHnuRkn8Q"
   },
   "source": [
    "注意，上面的张量值并没有变成 `tensor([110, 120, 130])`，这是因为张量中的值不会改变，除非它们被重新赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuB1UjCIEJIA",
    "outputId": "57cae862-c145-4681-d74b-fe6d77f2125a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量不会改变，除非重新赋值\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYvqGpUTk1o6"
   },
   "source": [
    "让我们减去一个数，这次我们将重新赋值给 `tensor` 变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4iWKoLsENry",
    "outputId": "14d6771d-eb57-4b11-88a7-b1bb308ddc6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行减法并重新赋值\n",
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFgZY-PaFNXa",
    "outputId": "3536ea54-a056-444c-cd5d-6d438ddda965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行加法并重新赋值\n",
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CYXDoIOzk-6I"
   },
   "source": [
    "PyTorch 还提供了一些内置函数，例如 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul)（乘法的缩写）和 [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html)，用于执行基本操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVysdk3kFWbY",
    "outputId": "3a5bf687-cf24-4224-9e76-975f84638ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以使用 PyTorch 函数\n",
    "torch.multiply(tensor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxuPJIpNFbqO",
    "outputId": "f04cafd9-eaea-4254-df1a-5ab3b524d74e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始张量依然未改变\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70UNL33AlVQq"
   },
   "source": [
    "然而，更常见的是使用运算符符号（如 `*`），而不是 `torch.mul()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5v3RkR0F2Jq",
    "outputId": "0137caab-5ea1-4d95-f4c5-a0baa0fd652d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13]) * tensor([11, 12, 13])\n",
      "Equals: tensor([121, 144, 169])\n"
     ]
    }
   ],
   "source": [
    "# 元素级乘法（每个元素与对应元素相乘，索引 0->0，1->1，2->2）\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5fVuyu7q5z"
   },
   "source": [
    "### 矩阵乘法（这是所有你需要的）\n",
    "\n",
    "在机器学习和深度学习算法（如神经网络）中，最常见的操作之一是[矩阵乘法](https://www.mathsisfun.com/algebra/matrix-multiplying.html)。\n",
    "\n",
    "PyTorch 通过 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) 方法实现矩阵乘法功能。\n",
    "\n",
    "矩阵乘法的两个主要规则是：\n",
    "\n",
    "1. **内维度**必须匹配：\n",
    "  * `(3, 2) @ (3, 2)` 不可行\n",
    "  * `(2, 3) @ (3, 2)` 可行\n",
    "  * `(3, 2) @ (2, 3)` 可行\n",
    "2. 结果矩阵的形状是**外维度**：\n",
    " * `(2, 3) @ (3, 2)` -> `(2, 2)`\n",
    " * `(3, 2) @ (2, 3)` -> `(3, 3)`\n",
    "\n",
    "> **注意:** 在 Python 中，\"`@`\" 是矩阵乘法的符号。\n",
    "\n",
    "> **资源:** 你可以在 [PyTorch 文档](https://pytorch.org/docs/stable/generated/torch.matmul.html) 中查看使用 `torch.matmul()` 的矩阵乘法规则。\n",
    "\n",
    "让我们创建一个张量，并对其进行元素级乘法和矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE7loucmDlEM",
    "outputId": "44032bf9-c1f7-42fc-c842-dbe7a5c1221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUAZ3_b0vOKv"
   },
   "source": [
    "元素级乘法和矩阵乘法的区别在于加和值的方式。\n",
    "\n",
    "对于 `tensor` 变量的值 `[1, 2, 3]`：\n",
    "\n",
    "| 操作 | 计算 | 代码 |\n",
    "| ----- | ----- | ----- |\n",
    "| **元素级乘法** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor` |\n",
    "| **矩阵乘法** | `[1*1 + 2*2 + 3*3]` = `[14]` | `tensor.matmul(tensor)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i42gkUeHvI_1",
    "outputId": "18a630ce-bb56-4c40-81b4-9fdbb2ed7a4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 元素级矩阵乘法\n",
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvCBiiTTDk8y",
    "outputId": "cf623247-8f1b-49f1-e788-16da3ed1e59c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4E_pROBDk2r",
    "outputId": "a09af00f-277b-479e-b0a2-ad6311ee5413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以使用 \"@\" 符号进行矩阵乘法，尽管不推荐这样做\n",
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obbginUMv43A"
   },
   "source": [
    "你可以手动进行矩阵乘法，但不推荐这样做。\n",
    "\n",
    "内置的 `torch.matmul()` 方法更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qMSaLOoJscL",
    "outputId": "8bcad8a2-c900-4966-e13c-ff2cc02b9207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 943 µs, sys: 1.18 ms, total: 2.13 ms\n",
      "Wall time: 1.23 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 手动进行矩阵乘法\n",
    "# （尽量避免使用for循环进行操作，它们在计算上非常昂贵）\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVWiKB0KwH74",
    "outputId": "fce58235-5c09-49ec-f34b-a90e5640281e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 µs, sys: 100 µs, total: 237 µs\n",
      "Wall time: 154 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ4DDmo1TGe-"
   },
   "source": [
    "## 深度学习中最常见的错误之一（形状错误）\n",
    "\n",
    "由于深度学习的大部分操作涉及矩阵相乘和其他矩阵运算，而矩阵在组合时有严格的形状和尺寸规则，因此在深度学习中，最常见的错误之一就是形状不匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rN5RcoD4Jo6y",
    "outputId": "20f6c65b-86f4-4903-d253-f6cbf0583934"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m tensor_A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      3\u001b[0m                          [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      4\u001b[0m                          [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m tensor_B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m      7\u001b[0m                          [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m11\u001b[39m], \n\u001b[1;32m      8\u001b[0m                          [\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m12\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 10\u001b[0m torch\u001b[38;5;241m.\u001b[39mmatmul(tensor_A, tensor_B)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNA6MZEFxWVt"
   },
   "source": [
    "我们可以通过使 `tensor_A` 和 `tensor_B` 的内维度匹配来使矩阵乘法得以执行。\n",
    "\n",
    "其中一种方法是使用 **转置**（交换给定张量的维度）。\n",
    "\n",
    "在 PyTorch 中，您可以使用以下两种方式进行转置：\n",
    "* `torch.transpose(input, dim0, dim1)` — 其中 `input` 是要转置的张量，`dim0` 和 `dim1` 是要交换的维度。\n",
    "* `tensor.T` — 其中 `tensor` 是要转置的张量。\n",
    "\n",
    "我们来试试后者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUqgaANiy1wq",
    "outputId": "e48bbf0c-8008-434e-d372-caa658b2f36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7., 10.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 查看 tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DveqxO7iy_Fi",
    "outputId": "1bd2e85b-ea4d-4948-c408-8eb46ef3534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 查看 tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35rEIu-NKtVE",
    "outputId": "0b32c7f1-556e-45d4-de22-388419e93dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 当 `tensor_B` 被转置时，操作就能正常进行\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfcFEqfLjN24"
   },
   "source": [
    "你也可以使用 [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html)，它是 `torch.matmul()` 的简写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3rJvW_TTGe_",
    "outputId": "2c501972-20bf-4a83-ad4a-b5f1b2424097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `torch.mm` 是 `matmul` 的快捷方式\n",
    "torch.mm(tensor_A, tensor_B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXKozI4T0hFi"
   },
   "source": [
    "没有转置时，矩阵乘法的规则不成立，导致出现上面的错误。\n",
    "\n",
    "如何用一个视觉效果来展示呢？\n",
    "\n",
    "![矩阵乘法的可视化演示](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/00-matrix-multiply-crop.gif)\n",
    "\n",
    "你可以在 [matrixmultiplication.xyz](http://matrixmultiplication.xyz/) 上创建自己的矩阵乘法可视化效果。\n",
    "\n",
    "> **注意：** 这样的矩阵乘法也被称为两个矩阵的 [**点积**](https://www.mathsisfun.com/algebra/vectors-dot-product.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA64Z4DmkB31"
   },
   "source": [
    "神经网络充满了矩阵乘法和点积。\n",
    "\n",
    "[`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) 模块（我们稍后会看到它的应用），也被称为前馈层或全连接层，实现了输入 `x` 和权重矩阵 `A` 之间的矩阵乘法。\n",
    "\n",
    "$$\n",
    "y = x\\cdot{A^T} + b\n",
    "$$\n",
    "\n",
    "其中：\n",
    "* `x` 是层的输入（深度学习是由多个层（如 `torch.nn.Linear()` 和其他层）堆叠而成的）。\n",
    "* `A` 是由层创建的权重矩阵，它最开始是一些随机数，随着神经网络的训练，它们会被调整，以便更好地表示数据中的模式（注意“`T`”，是因为权重矩阵被转置了）。\n",
    "  * **注意：** 你也可能常常看到 `W` 或其他字母（如 `X`）用来表示权重矩阵。\n",
    "* `b` 是偏置项，用来稍微调整权重和输入。\n",
    "* `y` 是输出（它是输入的变换，旨在从中发现模式）。\n",
    "\n",
    "这是一个线性函数（你可能在高中或其他地方见过类似 $y = mx+b$ 的表达式），它可以用来画一条直线！\n",
    "\n",
    "让我们玩一下线性层。\n",
    "\n",
    "尝试改变下面的 `in_features` 和 `out_features` 的值，看看会发生什么。\n",
    "\n",
    "你有没有注意到与形状有关的任何变化？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC_MjKW1LX7T",
    "outputId": "768f75d2-c978-4df3-e18a-4684d46bdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 由于线性层开始时使用随机权重矩阵，设定随机种子以确保结果可复现（稍后会详细讲解）\n",
    "torch.manual_seed(42)\n",
    "# 这是使用矩阵乘法\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = 输入的内维度与矩阵相乘的维度\n",
    "                         out_features=6) # out_features = 输出的外维度，描述最终的输出大小\n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIGrP5j1pN7j"
   },
   "source": [
    "> **问题：** 如果你将上面的 `in_features` 从 2 改为 3，会发生什么？是否会报错？你可以如何更改输入（`x`）的形状以适应这个错误？提示：我们之前是如何处理 `tensor_B` 的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPNF0nMWoGEj"
   },
   "source": [
    "如果你以前没有做过，矩阵乘法一开始可能会让人感到困惑。\n",
    "\n",
    "但在你玩弄了几次之后，甚至深入了解了一些神经网络，你会发现它无处不在。\n",
    "\n",
    "记住，矩阵乘法就是你所需要的一切。\n",
    "\n",
    "![矩阵乘法就是你所需要的一切](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)\n",
    "\n",
    "*当你开始深入研究神经网络层并构建自己的网络时，你会发现矩阵乘法无处不在。**来源：** https://marksaroufim.substack.com/p/working-class-deep-learner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMmrJOOPv5e"
   },
   "source": [
    "### 查找最小值、最大值、均值、总和等（聚合）\n",
    "\n",
    "现在我们已经看到了一些操作张量的方法，让我们来看一下几种聚合张量的方法（从更多的值变到更少的值）。\n",
    "\n",
    "首先，我们将创建一个张量，然后找到它的最大值、最小值、均值和总和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrFQbe5fP1Rk",
    "outputId": "034013c1-b384-4a0d-edf8-295ed3a456f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量\n",
    "x = torch.arange(0, 100, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J-wfMdlsEco"
   },
   "source": [
    "现在让我们进行一些聚合操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5wSP9YKP3Lb",
    "outputId": "3aa238c7-646f-434f-a55c-292aabef7227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # 这会报错\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # 如果没有使用 float 数据类型，将无法工作\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHoKpsg3sKQE"
   },
   "source": [
    "> **注意：** 你可能会发现一些方法，如 `torch.mean()`，要求张量的类型为 `torch.float32`（最常见的类型）或其他特定数据类型，否则操作将失败。\n",
    "\n",
    "你也可以使用 `torch` 方法执行与上述相同的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cr23Y9uP3HO",
    "outputId": "9c86d805-eef2-465c-e2c8-2bccd515e6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(0), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7ApCaZjDkvp"
   },
   "source": [
    "### 位置最小值/最大值\n",
    "\n",
    "你还可以使用 [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) 和 [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) 来找到张量中最大值或最小值出现的索引。\n",
    "\n",
    "这在你只关心最高（或最低）值的位置而不是实际数值时非常有用（稍后我们会在使用 [softmax 激活函数](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) 时看到这一点）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzNBl9JSGlHi",
    "outputId": "01e0740e-c34f-469b-9c8f-9e6e5f0363af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "# 返回最大值和最小值的索引\n",
    "print(f\"Index where max value occurs: {tensor.argmax()}\")\n",
    "print(f\"Index where min value occurs: {tensor.argmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBu33WihOXBk"
   },
   "source": [
    "### 更改张量数据类型\n",
    "\n",
    "如前所述，深度学习操作中常见的问题之一是张量的数据类型不一致。\n",
    "\n",
    "如果一个张量是 `torch.float64` 类型，而另一个是 `torch.float32` 类型，就可能会遇到一些错误。\n",
    "\n",
    "但有一个解决方法。\n",
    "\n",
    "你可以使用 [`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html) 来更改张量的数据类型，其中 `dtype` 参数是你想要使用的数据类型。\n",
    "\n",
    "首先，我们创建一个张量并检查它的数据类型（默认是 `torch.float32`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY2FEsCAOaLu",
    "outputId": "507f1ade-7c7a-4172-fa48-60c9ac4831c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量并检查它的数据类型\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR30FHEc92of"
   },
   "source": [
    "现在我们将创建另一个与之前相同的张量，但将其数据类型更改为 `torch.float16`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cac8gRYjOeab",
    "outputId": "96e5ce12-bc29-4a2b-f81c-bfc89ea2d075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个 float16 张量\n",
    "tensor_float16 = tensor.type(torch.float16)\n",
    "tensor_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndVlKJZ4-7_5"
   },
   "source": [
    "And we can do something similar to make a `torch.int8` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Yqovld2Oj6s",
    "outputId": "667da17f-e38f-404a-bd2d-63683e45c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个 int8 张量\n",
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44GxVabar-xe"
   },
   "source": [
    "> **注意：** 一开始，不同的数据类型可能会让人感到困惑。但可以这样理解，数字越小（例如 32、16、8），计算机存储值时的精度就越低。存储需求较低通常会导致计算速度更快，模型整体也更小。基于移动设备的神经网络通常使用 8 位整数，运行更小更快，但精度低于 32 位浮点数。如果想了解更多，可以阅读关于[计算中的精度](https://en.wikipedia.org/wiki/Precision_(computer_science))的内容。\n",
    "\n",
    "> **练习：** 到目前为止，我们已经覆盖了很多张量方法，但在[`torch.Tensor`文档](https://pytorch.org/docs/stable/tensors.html)中还有许多方法。我建议花 10 分钟浏览一遍，看看有没有吸引你的方法。点击它们并尝试编写代码，看看会发生什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CkCtAYmGsHY"
   },
   "source": [
    "### 重新形状、堆叠、压缩和扩展\n",
    "\n",
    "在深度学习中，很多时候你需要在不改变张量内部数值的情况下重新调整张量的形状或改变其维度。\n",
    "\n",
    "为了实现这个目标，一些常用的方法如下：\n",
    "\n",
    "| 方法 | 简要描述 |\n",
    "| ----- | ----- |\n",
    "| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | 将 `input` 重塑为 `shape`（如果兼容），也可以使用 `torch.Tensor.reshape()`。 |\n",
    "| [`Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | 返回原始张量的新视图，形状为 `shape`，但与原始张量共享相同的数据。 |\n",
    "| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | 在新维度（`dim`）上连接一系列 `tensors`，所有 `tensors` 必须具有相同的大小。 |\n",
    "| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | 压缩 `input`，去除所有值为 `1` 的维度。 |\n",
    "| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | 返回 `input`，在 `dim` 维度处增加一个值为 `1` 的维度。 |\n",
    "| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | 返回原始 `input` 的一个 *视图*，其维度根据 `dims` 被重新排列。 |\n",
    "\n",
    "这些方法为什么重要？\n",
    "\n",
    "因为深度学习模型（神经网络）主要是通过某种方式操作张量。而矩阵乘法的规则要求，如果张量形状不匹配，你会遇到错误。这些方法帮助确保张量的正确元素与其他张量的正确元素相结合。\n",
    "\n",
    "接下来我们来尝试这些方法。\n",
    "\n",
    "首先，我们将创建一个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYjRTLOzG4Ev",
    "outputId": "f7f2719c-15ce-406b-dc8f-4477046cd5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量\n",
    "import torch\n",
    "x = torch.arange(1., 8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_VarMO9CoT8"
   },
   "source": [
    "现在我们使用 `torch.reshape()` 添加一个额外的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US4WjpQ3SG-8",
    "outputId": "c519d59e-85f1-4a10-eaaa-acb487028e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加一个额外的维度\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "x_reshaped, x_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tig5xm0jCxuU"
   },
   "source": [
    "我们还可以使用 `torch.view()` 来改变视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDN2BNe5TGfB",
    "outputId": "3df1b0d6-2548-4ecc-ca25-0c4e28a6e536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改变视图（保持与原始数据相同，但改变视图）\n",
    "# 参见更多内容: https://stackoverflow.com/a/54507446/7900723\n",
    "z = x.view(1, 7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8joAaUEC2NX"
   },
   "source": [
    "不过请记住，通过 `torch.view()` 改变张量的视图实际上只是创建了一个*相同*张量的新视图。\n",
    "\n",
    "因此，改变视图也会改变原始张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DxURVvXTGfC",
    "outputId": "668d194d-dd0a-4db1-da00-9c3fd8849186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改变 z 会改变 x\n",
    "z[:, 0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxnqDBlpDDJ_"
   },
   "source": [
    "如果我们想将新的张量堆叠五次，可以使用 `torch.stack()` 来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pX5Adf3ORiTK",
    "outputId": "703e8568-61df-4ebd-f4d3-a6366dc265c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将张量堆叠在一起\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0)  # 尝试将 dim 改为 dim=1 看看会发生什么\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET56QzNHDuOI"
   },
   "source": [
    "如何去除张量中的所有单一维度？\n",
    "\n",
    "可以使用 `torch.squeeze()` 来实现这一点（我记得它是通过“挤压”张量，使其只保留大于 1 的维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2Y2HEoDRxJZ",
    "outputId": "dd0645a6-1cdd-46bc-a3a2-433d9cd09336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "# 从 `x_reshaped` 中移除额外的维度\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acjDLk8WD8NC"
   },
   "source": [
    "要执行 `torch.squeeze()` 的反向操作，可以使用 `torch.unsqueeze()` 在指定的索引处添加一个值为 1 的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUC-DEEwSYv7",
    "outputId": "da60e019-3ea6-42f8-8e47-ba037ead737f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Previous shape: torch.Size([7])\n",
      "\n",
      "New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "New shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "## 使用 `torch.unsqueeze()` 添加一个额外的维度\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"\\nNew tensor: {x_unsqueezed}\")\n",
    "print(f\"New shape: {x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9DuJzXgFbM5"
   },
   "source": [
    "你还可以使用 `torch.permute(input, dims)` 来重新排列轴的顺序，其中 `input` 会变成一个具有新维度顺序的 *view*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCRGCX8DTGfC",
    "outputId": "6853328b-a1cf-4470-f366-106a231a189c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 创建具有特定形状的张量\n",
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "# 调整原始张量的轴顺序\n",
    "x_permuted = x_original.permute(2, 0, 1)  # 将轴0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LKaFemGBoE"
   },
   "source": [
    "> **注意**：由于 `permute` 返回的是一个 *视图*（与原始张量共享相同的数据），因此在 permuted 张量中的值与原始张量相同。如果你改变了视图中的值，它也会改变原始张量的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEPqVL7fTGfC"
   },
   "source": [
    "## 索引（从张量中选择数据）\n",
    "\n",
    "有时你需要从张量中选择特定的数据（例如，选择第一列或第二行）。\n",
    "\n",
    "为此，你可以使用索引。\n",
    "\n",
    "如果你曾经在 Python 列表或 NumPy 数组上进行过索引，那么在 PyTorch 中对张量进行索引也是非常相似的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSXzdxCQTGfD",
    "outputId": "05a72c08-5f8c-433a-cd31-46065686f825"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量\n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQG5krnKG43B"
   },
   "source": [
    "索引值是从外部维度到内部维度（查看方括号）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zv_Z3IAzTGfD",
    "outputId": "cf6c0936-7600-4af4-9b6f-f6b8ac9b4c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个方括号:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "第二个方括号: tensor([1, 2, 3])\n",
      "第三个方括号: 1\n"
     ]
    }
   ],
   "source": [
    "# 让我们逐层索引\n",
    "print(f\"第一个方括号:\\n{x[0]}\") \n",
    "print(f\"第二个方括号: {x[0][0]}\") \n",
    "print(f\"第三个方括号: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaLjaIFxHe89"
   },
   "source": [
    "You can also use `:` to specify \"all values in this dimension\" and then use a comma (`,`) to add another dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCT09pqeTGfD",
    "outputId": "a91f9b73-f8f0-476a-9c69-fcd03b042f6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取第0维的所有值和第1维第0索引的值\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwDx_gMsTGfD",
    "outputId": "8165cfd9-a88d-4212-8c45-1eb84ef5be83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取第0维和第1维的所有值，但仅获取第2维的索引1的值\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiw3_1E3TGfD",
    "outputId": "12fa4749-cf52-4e88-c2c0-44d26aeb633c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取第0维的所有值，但仅获取第1维和第2维索引为1的值\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFVEgrKhTGfD",
    "outputId": "69eadeb9-11b3-4b48-cb95-0b3305c1274c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取第0维和第1维索引为0的值，以及第2维的所有值\n",
    "x[0, 0, :]  # 与 x[0][0] 相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ik0r11RIxtm"
   },
   "source": [
    "索引刚开始可能会有些混乱，尤其是对于更大的张量（我自己也经常需要多次尝试索引才能正确）。但是通过一些练习，并遵循数据探查者的座右铭（***可视化，可视化，可视化***），你会开始掌握它的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ZaW0Bq7rCm"
   },
   "source": [
    "## PyTorch 张量与 NumPy\n",
    "\n",
    "由于 NumPy 是一个流行的 Python 数值计算库，PyTorch 提供了与其良好互动的功能。\n",
    "\n",
    "你将要使用的两个主要方法，用于在 NumPy 和 PyTorch 之间转换（及反向转换）是：\n",
    "* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - 将 NumPy 数组转换为 PyTorch 张量。\n",
    "* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - 将 PyTorch 张量转换为 NumPy 数组。\n",
    "\n",
    "让我们试试看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDrDCnvY7rKS",
    "outputId": "86155a63-01f9-4372-e889-61a65ebf0fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "import torch\n",
    "import numpy as np\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16JG6cONLPnO"
   },
   "source": [
    "> **注意：** 默认情况下，NumPy 数组的创建数据类型是 `float64`，如果将其转换为 PyTorch 张量，它将保持相同的数据类型（如上所示）。  \n",
    "> 然而，许多 PyTorch 计算默认使用 `float32` 数据类型。  \n",
    ">\n",
    "> 因此，如果你想将 NumPy 数组（`float64`）-> PyTorch 张量（`float64`）-> PyTorch 张量（`float32`），可以使用 `tensor = torch.from_numpy(array).type(torch.float32)`。\n",
    "\n",
    "因为我们上面重新分配了 `tensor`，所以如果你更改张量，数组将保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovwl7VCREv8L",
    "outputId": "efd21eb9-0010-436a-dc29-f851e3d7d77a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 更改数组，保持张量不变\n",
    "array = array + 1  \n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geVvu1p0MTWc"
   },
   "source": [
    "And if you want to go from PyTorch tensor to NumPy array, you can call `tensor.numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw_7ZyVaTKxQ",
    "outputId": "54d6f347-d3f6-44df-9155-83d980c31780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量转换为NumPy数组\n",
    "tensor = torch.ones(7)  # 创建一个值为1的张量，数据类型为float32\n",
    "numpy_tensor = tensor.numpy()  # 默认数据类型为float32，除非修改\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt8yEV1jMfi2"
   },
   "source": [
    "同样的规则适用，如果你改变了原始的 `tensor`，新的 `numpy_tensor` 会保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMp6ZSkET4_Y",
    "outputId": "100678a4-c220-4a44-e4a5-0542359cb9de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改变张量，保持数组不变\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gU3ubCrUkI-"
   },
   "source": [
    "## 可重复性（尽量消除随机性）\n",
    "\n",
    "当你深入学习神经网络和机器学习时，你会开始发现随机性在其中扮演着多么重要的角色。\n",
    "\n",
    "实际上，是伪随机性。毕竟，计算机从根本上是确定性的（每一步都是可以预测的），因此它们创造的随机性是模拟的随机性（尽管对此有争议，但既然我不是计算机科学家，就让你自己去了解更多）。\n",
    "\n",
    "那么，这与神经网络和深度学习有什么关系呢？\n",
    "\n",
    "我们已经讨论过，神经网络从随机数开始，用来描述数据中的模式（这些数字描述得很差），然后尝试通过张量操作（以及一些我们尚未讨论的内容）来改进这些随机数，以更好地描述数据中的模式。\n",
    "\n",
    "简而言之：\n",
    "\n",
    "`` 从随机数开始 -> 张量操作 -> 尝试更好地描述（一次又一次）``\n",
    "\n",
    "虽然随机性很有用且强大，但有时候你希望随机性能少一点。\n",
    "\n",
    "为什么呢？\n",
    "\n",
    "这样你就可以进行可重复的实验。\n",
    "\n",
    "举个例子，你创建了一个能够达到X性能的算法。\n",
    "\n",
    "然后你的朋友试一下，验证你不是疯了。\n",
    "\n",
    "他们怎么做到的呢？\n",
    "\n",
    "这就是**可重复性**的问题所在。\n",
    "\n",
    "换句话说，你能在你的计算机上运行相同的代码，得到的结果与我得到的非常相似（或完全相同）吗？\n",
    "\n",
    "让我们看一个在 PyTorch 中关于可重复性的简单示例。\n",
    "\n",
    "我们首先创建两个随机张量，因为它们是随机的，你会期待它们不同，对吧？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSwxnwEbTGfF",
    "outputId": "73b34154-734f-496f-9b55-b6aaa137e854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n",
      "        [0.7687, 0.4566, 0.5745, 0.9200],\n",
      "        [0.3230, 0.8613, 0.0919, 0.3102]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n",
      "        [0.3743, 0.5220, 0.1336, 0.9666],\n",
      "        [0.9754, 0.8474, 0.8988, 0.1105]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个随机张量\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPU6mDKJnr8M"
   },
   "source": [
    "正如你所预期的那样，这两个张量的值是不同的。\n",
    "\n",
    "但如果你希望创建两个拥有*相同*值的随机张量呢？\n",
    "\n",
    "换句话说，张量仍然会包含随机值，但它们会具有相同的“风味”。\n",
    "\n",
    "这时，[`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) 就派上用场了，其中 `seed` 是一个整数（像 `42`，但可以是任何值），它为随机性提供了“风味”。\n",
    "\n",
    "让我们尝试一下，通过创建一些具有相同“风味”的随机张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sB6d1GfYTGfF",
    "outputId": "4d11d38e-4406-4aff-9a81-cf13aa89ee5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# # 设置随机种子\n",
    "RANDOM_SEED = 42  # 尝试将其更改为不同的值，看看下面的数字会发生什么变化\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# 每次调用新的 rand() 时都需要重置种子\n",
    "# 如果没有这一行，tensor_D 将与 tensor_C 不同\n",
    "torch.random.manual_seed(seed=RANDOM_SEED)  # 尝试注释掉这一行，看看会发生什么\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uct53Xr5QRC_"
   },
   "source": [
    "太好了！\n",
    "\n",
    "看起来设置种子确实起作用了。\n",
    "\n",
    "> **资源推荐：** 我们刚才讨论的只是 PyTorch 中可重复性的表面。对于更多关于可重复性和随机种子的内容，可以参考以下资料：\n",
    "> * [PyTorch 可重复性文档](https://pytorch.org/docs/stable/notes/randomness.html)（一个好的练习是花 10 分钟阅读这篇文档，即使现在理解不了，了解这些概念也很重要）。\n",
    "> * [维基百科上的随机种子页面](https://en.wikipedia.org/wiki/Random_seed)（这将提供关于随机种子和伪随机性的概述）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxIIM7t27rQ-"
   },
   "source": [
    "## 在GPU上运行张量（加速计算）\n",
    "\n",
    "深度学习算法需要进行大量的数值运算。\n",
    "\n",
    "默认情况下，这些运算通常是在CPU（中央处理单元）上执行的。\n",
    "\n",
    "然而，还有另一种常见的硬件，叫做GPU（图形处理单元），它在执行神经网络所需的特定类型的运算（矩阵乘法）时，比CPU要快得多。\n",
    "\n",
    "你的计算机可能配有GPU。\n",
    "\n",
    "如果有GPU，建议你尽可能利用它来训练神经网络，因为它通常能显著加速训练过程。\n",
    "\n",
    "首先，你需要获得对GPU的访问权限，然后让PyTorch使用GPU。\n",
    "\n",
    "> **注意：** 当我在本课程中提到“GPU”时，指的是启用了[CUDA](https://developer.nvidia.com/cuda-gpus)的Nvidia GPU（CUDA是一个计算平台和API，帮助GPU不仅用于图形渲染，还能用于一般计算），除非特别说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UiR6QpoYQH_"
   },
   "source": [
    "### 1. 获取GPU\n",
    "\n",
    "你可能已经知道我提到GPU时的含义。如果不清楚，以下是几种获取GPU的方法。\n",
    "\n",
    "| **方法** | **设置难度** | **优点** | **缺点** | **如何设置** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| Google Colab | 简单 | 免费使用，几乎不需要设置，可以像分享链接一样与他人共享工作 | 不会保存数据输出，计算资源有限，可能会超时 | [参照Google Colab指南](https://colab.research.google.com/notebooks/gpu.ipynb) |\n",
    "| 自己的GPU | 中等 | 所有工作都可以在自己的机器上本地运行 | GPU不是免费的，需要预先购买 | 参照[PyTorch本地安装指南](https://pytorch.org/get-started/locally/) |\n",
    "| 云计算（AWS, GCP, Azure） | 中等-难 | 小额的前期成本，几乎无限的计算资源 | 长时间运行时可能会很昂贵，设置较为复杂 | 参照[PyTorch云计算安装指南](https://pytorch.org/get-started/cloud-partners/) |\n",
    "\n",
    "还有其他使用GPU的选项，但以上三种方法足以满足基本需求。\n",
    "\n",
    "就我个人而言，我在进行小规模实验（以及制作这门课程）时使用Google Colab和个人计算机的组合，必要时会选择云计算资源来获取更多的计算能力。\n",
    "\n",
    "> **资源：** 如果你打算购买自己的GPU，但不确定选什么，[Tim Dettmers有一篇很棒的指南](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)。\n",
    "\n",
    "要检查是否有Nvidia GPU的访问权限，可以运行 `!nvidia-smi`，其中 `!`（也叫做bang）表示“在命令行中运行”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEMcO-9zYc-w",
    "outputId": "77405db7-3494-4add-cfc7-8415e52a0412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvkB9p5zYf8E"
   },
   "source": [
    "如果你没有访问到Nvidia GPU，上面的命令会输出类似以下内容：\n",
    "\n",
    "```\n",
    "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
    "```\n",
    "\n",
    "在这种情况下，请返回并按照安装步骤进行操作。\n",
    "\n",
    "如果你确实有GPU，以上命令会输出类似如下内容：\n",
    "\n",
    "```\n",
    "Wed Jan 19 22:09:08 2022       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvibZ6e0YcDk"
   },
   "source": [
    "### 2. 让 PyTorch 在 GPU 上运行\n",
    "\n",
    "一旦你有了可以访问的 GPU，接下来的步骤就是让 PyTorch 使用它来存储数据（张量）并对数据进行计算（执行张量操作）。\n",
    "\n",
    "为此，你可以使用 [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) 包。\n",
    "\n",
    "与其详细解释，我们不如直接尝试一下。\n",
    "\n",
    "你可以通过 [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available) 来测试 PyTorch 是否可以访问 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OweDLgwjEvZ2",
    "outputId": "3a278a24-3ec3-4b1f-8f96-298086fa6ea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查 GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jedZcx2PZFpL"
   },
   "source": [
    "如果上面的输出是 `True`，说明 PyTorch 可以访问并使用 GPU；如果输出是 `False`，则说明 PyTorch 无法访问 GPU，这时你需要回去重新检查安装步骤。\n",
    "\n",
    "现在，假设你想设置代码，使其在 CPU 上运行，如果 GPU 可用则运行在 GPU 上。\n",
    "\n",
    "这样，无论你自己还是其他人运行你的代码，它都会根据所使用的计算设备自动选择。\n",
    "\n",
    "让我们创建一个 `device` 变量来存储当前可用的设备类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j92HBCKB7rYa",
    "outputId": "8cca1643-645c-4b67-f1f5-37066f6b9549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置设备类型\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjFyPP2WaCch"
   },
   "source": [
    "如果上面的输出是 `\"cuda\"`，这意味着我们可以将所有的 PyTorch 代码设置为使用可用的 CUDA 设备（即 GPU）；如果输出是 `\"cpu\"`，那么 PyTorch 代码将继续使用 CPU。\n",
    "\n",
    "> **注意：** 在 PyTorch 中，最佳实践是编写 [**设备无关代码**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code)，也就是说编写可以在 CPU（始终可用）或 GPU（如果可用）上运行的代码。\n",
    "\n",
    "如果你想加速计算，可以使用 GPU，但如果你想做 *更* 快的计算，可以使用多个 GPU。\n",
    "\n",
    "你可以使用 [`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count) 来统计 PyTorch 可以访问的 GPU 数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MArsn0DFTGfG",
    "outputId": "de717df5-bb67-4900-805e-a6f00ad0b409"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计设备数量\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVNf1hiqa-gO"
   },
   "source": [
    "知道 PyTorch 可以访问的 GPU 数量很有用，以防你想在一个 GPU 上运行一个特定的过程，而在另一个 GPU 上运行另一个过程（PyTorch 还具有让你在 *所有* GPU 上运行过程的功能）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQLcuj68OA-"
   },
   "source": [
    "### 3. 将张量（和模型）放到 GPU 上\n",
    "\n",
    "你可以通过对张量（和模型，我们稍后会看到）调用 [`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) 方法，将它们放置到特定的设备上，其中 `device` 是你希望张量（或模型）去的目标设备。\n",
    "\n",
    "为什么要这样做？\n",
    "\n",
    "GPU 提供比 CPU 快得多的数值计算速度，如果 GPU 不可用，由于我们的 **设备无关代码**（见上文），它将运行在 CPU 上。\n",
    "\n",
    "> **注意：** 使用 `to(device)` 将张量放到 GPU 上（例如：`some_tensor.to(device)`）会返回该张量的副本，也就是说，同一个张量将同时存在于 CPU 和 GPU 上。如果要覆盖张量，需要重新赋值：\n",
    ">\n",
    "> `some_tensor = some_tensor.to(device)`\n",
    "\n",
    "让我们尝试创建一个张量并将其放到 GPU 上（如果可用的话）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhI3srFXEHfP",
    "outputId": "2f4f6435-fdc4-4e99-e87c-9421c2100f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建张量（默认在 CPU 上）\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 张量不在 GPU 上\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# 将张量移动到 GPU 上（如果可用的话）\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXeRKO0TGfG"
   },
   "source": [
    "如果你有可用的 GPU，上面的代码将输出如下内容：\n",
    "\n",
    "```\n",
    "tensor([1, 2, 3]) cpu\n",
    "tensor([1, 2, 3], device='cuda:0')\n",
    "```\n",
    "\n",
    "注意第二个张量显示了 `device='cuda:0'`，这意味着它存储在第 0 个 GPU 上（GPU 是从 0 开始索引的，如果有两个 GPU，它们分别会是 `'cuda:0'` 和 `'cuda:1'`，最多到 `'cuda:n'`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4puyUX4Bci5D"
   },
   "source": [
    "### 4. 将张量移回 CPU\n",
    "\n",
    "如果我们想将张量移回 CPU，怎么办呢？\n",
    "\n",
    "例如，如果你想使用 NumPy 与张量进行交互时，你需要将其移回 CPU（因为 NumPy 不支持 GPU）。\n",
    "\n",
    "让我们尝试在 `tensor_on_gpu` 上使用 [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "3ChSLJgPTGfG",
    "outputId": "32e92f62-db28-4dc7-ce93-c2ab33229252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果张量在 GPU 上，无法将其转换为 NumPy（这将报错）\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhymtkRDTGfG"
   },
   "source": [
    "相反，为了将张量移回 CPU 并使其可以与 NumPy 一起使用，我们可以使用 [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html)。\n",
    "\n",
    "这将张量复制到 CPU 内存中，从而使其可以在 CPU 上使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN15s-NdTGfG",
    "outputId": "9fffb6f2-c200-4f9c-d987-d9ab5d9cba49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相反，将张量复制回 CPU\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyzNH5lrTGfH"
   },
   "source": [
    "上面的操作返回了一个在 CPU 内存中的 GPU 张量副本，因此原始张量仍然保留在 GPU 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5u83PCRTGfH",
    "outputId": "4cb931e2-7c8d-49b9-a7de-db3d3c6589b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlmBpnuPTGfH"
   },
   "source": [
    "## 练习\n",
    "\n",
    "所有练习都集中在实践上面提到的代码。\n",
    "\n",
    "你可以通过参考每一部分或跟随链接的资源来完成这些练习。\n",
    "\n",
    "**资源：**\n",
    "\n",
    "* [00 练习模板笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb)\n",
    "* [00 示例解决方案笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb)（在查看之前，先尝试做这些练习）\n",
    "\n",
    "1. **文档阅读** - 深度学习（以及学习编程的一部分）中一个重要的部分是熟悉你正在使用的框架的文档。在本课程的其余部分，我们将频繁使用 PyTorch 文档。所以建议你花 10 分钟阅读以下内容（如果现在不理解某些内容也没关系，重点是提高意识，而不是完全理解）。请阅读 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) 和 [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics) 的文档。\n",
    "2. 创建一个形状为 `(7, 7)` 的随机张量。\n",
    "3. 对第二步中的张量和另一个形状为 `(1, 7)` 的随机张量进行矩阵乘法（提示：你可能需要对第二个张量进行转置）。\n",
    "4. 将随机种子设置为 `0`，然后重复进行练习 2 和 3。\n",
    "5. 说到随机种子，我们看到了如何使用 `torch.manual_seed()` 设置种子，但是否有 GPU 相关的设置方法？（提示：你需要查看 `torch.cuda` 的文档）。如果有，将 GPU 随机种子设置为 `1234`。\n",
    "6. 创建两个形状为 `(2, 3)` 的随机张量，并将它们都发送到 GPU（你需要访问 GPU 才能完成）。创建张量时，使用 `torch.manual_seed(1234)` 设置种子（这不一定是 GPU 随机种子）。\n",
    "7. 对第 6 步中创建的张量进行矩阵乘法（同样，你可能需要调整其中一个张量的形状）。\n",
    "8. 找出第 7 步输出的最大值和最小值。\n",
    "9. 找出第 7 步输出的最大值和最小值的索引。\n",
    "10. 创建一个形状为 `(1, 1, 1, 10)` 的随机张量，然后去掉所有的 `1` 维度，得到形状为 `(10)` 的新张量。在创建张量时将种子设置为 `7`，并打印出第一个张量及其形状以及第二个张量及其形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlmBpnuPTGfH"
   },
   "source": [
    "## 课外学习\n",
    "\n",
    "* 花 1 小时浏览 [PyTorch 基础教程](https://pytorch.org/tutorials/beginner/basics/intro.html)（我推荐查看 [快速入门](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) 和 [张量](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) 部分）。\n",
    "* 如果想了解更多关于张量如何表示数据的信息，可以观看这个视频：[什么是张量？](https://youtu.be/f5liqUk0ZTw)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "00_pytorch_fundamentals.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
